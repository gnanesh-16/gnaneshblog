<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en_US"><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/gnaneshblog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/gnaneshblog/" rel="alternate" type="text/html" hreflang="en_US" /><updated>2025-11-06T21:24:48+05:30</updated><id>http://localhost:4000/gnaneshblog/feed.xml</id><title type="html">Gnanesh Balusa blog</title><subtitle>Software Developer sharing insights on technology, programming, and my journey in software development.</subtitle><author><name>Gnanesh Balusa</name></author><entry><title type="html">Skip ChatGPT Forever: Run Your Own AI on Any Laptop for Free</title><link href="http://localhost:4000/gnaneshblog/ai/2025/11/06/Skip-ChatGPT-Forever-Run-Your-Own-AI-on-Any-Laptop-for-Free.html" rel="alternate" type="text/html" title="Skip ChatGPT Forever: Run Your Own AI on Any Laptop for Free" /><published>2025-11-06T21:18:00+05:30</published><updated>2025-11-06T21:18:00+05:30</updated><id>http://localhost:4000/gnaneshblog/ai/2025/11/06/Skip%20ChatGPT%20Forever%20Run%20Your%20Own%20AI%20on%20Any%20Laptop%20for%20Free</id><content type="html" xml:base="http://localhost:4000/gnaneshblog/ai/2025/11/06/Skip-ChatGPT-Forever-Run-Your-Own-AI-on-Any-Laptop-for-Free.html"><![CDATA[<p>You’re working late. Another ChatGPT bill notification hits your email. Two hundred bucks. This month. You’ve been running it for code reviews, document summaries, customer analysis, the usual stuff. Nothing wild. Your coworker glances over. “We still paying for that?” You shrug. Because honestly, you never thought there was another way.</p>

<p>Except there is. And it’s literally sitting on your machine right now.</p>

<p>What if you could run a real, capable language model directly on your computer. Completely offline. Free. Forever. No cloud services to worry about. No wondering if your data’s being analyzed somewhere else. No API rate limits. No shock bills at the end of the month. Just you and an AI that’s actually yours.This used to sound impossible. Running local LLMs meant wrestling with Linux terminals, CUDA drivers, dependency hell for hours. But something shifted. Tools like Ollama and LM Studio made this stupid easy. You don’t need to be a machine learning expert anymore. You need fifteen minutes and a willingness to try something different.</p>

<h2 id="why-you-should-actually-care">Why You Should Actually Care</h2>

<p>Let’s be straight about this. When you use ChatGPT, your prompts go to OpenAI’s servers. Every question you ask. Every code snippet. Every thought you type. If you’re working with anything proprietary, sensitive, confidential, financial you’re taking a risk. Even if OpenAI isn’t malicious, the surface area for data exposure exists. And in regulated industries, this isn’t just uncomfortable. It’s sometimes illegal.</p>

<p>But there’s something deeper than privacy. Using cloud AI creates constant friction. ChatGPT Plus is twenty bucks monthly. The API charges per token. Every time you experiment, prototype, test something new, there’s this background awareness that you’re spending money. Running local changes that entirely. You get unlimited usage. Zero throttling. Zero costs beyond your hardware investment.</p>

<p>Here’s what you actually gain running local:</p>

<ul>
  <li>
    <p>Your data stays yours. Not on someone else’s servers. Not in someone else’s logs. On your machine. Period.</p>
  </li>
  <li>
    <p>It works offline. No internet? Still works. Traveling? Still works. Server down? Doesn’t matter. The model runs.</p>
  </li>
  <li>
    <p>Use it as much as you want. No API limits. No rate limiting. Fire up a thousand requests if you need to. Nothing stops you.</p>
  </li>
  <li>
    <p>Customize it however you need. Fine tune models on your internal docs. Train them on your codebase. Make them understand your specific needs. Cloud APIs never let you do this.</p>
  </li>
  <li>
    <p>Stop paying forever. One time hardware cost. That’s it. Compare that to year after year of subscriptions. The break even point hits fast.</p>
  </li>
</ul>

<p>Yeah, a good GPU costs maybe fifteen hundred bucks upfront. But if you’re a developer who uses AI regularly, that pays for itself in months. Someone paying twenty a month for ChatGPT Plus plus API costs? That’s two hundred forty a year. Times that across a team and suddenly hardware looks ridiculously cheap.</p>

<h2 id="the-models-are-actually-good-now">The Models Are Actually Good Now</h2>

<p>This matters because the model quality directly determines if this is worth doing. Two years ago, local options were pretty weak. Today? Everything changed.<strong>Mistral 7B</strong> is the model that made people pay attention. It’s seven billion parameters. Sounds massive until you realize it actually beats Llama 2’s thirteen billion parameter version on most benchmarks. And it fits on basically any modern laptop. Runs fast. Generates thoughtful responses. Code, reasoning, creative writing, analysis it handles everything.</p>

<p>Then Meta released <strong>Llama 3.2</strong>. Different sizes depending on what your hardware can handle. One billion for tiny devices. Three billion for older machines. Eleven billion if you want real power. And ninety billion if you’re building servers. And here’s the kicker: the larger versions handle images. Show them a screenshot, a diagram, a photo. They understand it. That’s capability that was impossible locally not that long ago.</p>

<p><strong>DeepSeek</strong> just dropped V3.2. Open source. Reasoning that rivals ChatGPT. Dramatically smaller file sizes. Downloads directly to your machine. The benchmark numbers are actually wild. Mistral 7B gets around sixty percent accuracy on massive multitask language understanding. The bigger Mistral versions hit seventy percent. Compare that to proprietary models that cost money every single time you run them. Not just close. Actually competitive.</p>

<h2 id="two-tools-make-this-possible">Two Tools Make This Possible</h2>

<p>The revolution happened because two tools emerged that actually understood what users needed. Ollama and LM Studio.</p>

<p><strong>Ollama</strong> is lightweight and minimal. Command line focused but don’t let that scare you. It’s literally two commands. Download. Run. That’s your setup. It supports every popular open source model. Llama. Mistral. Gemma. Everything. And because it’s so minimal, it runs on absolutely anything. M1 Mac. Old Windows laptop. Raspberry Pi. Doesn’t matter. It just works.Plus Ollama has a built in REST API. Run it once in the background. Connect other applications to it. Build chatbots. Integrate it into workflows. Suddenly you’ve got serious infrastructure running locally.</p>

<p><strong>LM Studio</strong> is the friendlier version. Open the GUI. Search for a model. Click download. Chat with it immediately. Beautiful interface. No terminal required. It’s got built in RAG support which means you feed it your own documents and ask questions. Your internal knowledge base becomes searchable. It’s got integrations for developers. The onboarding is honestly the smoothest local LLM experience I’ve seen.</p>

<p>Both tools work together perfectly. Run Ollama in the background. Use LM Studio as your interface. Or use LM Studio’s server capabilities to power your apps. The whole ecosystem got genuinely good.</p>

<h2 id="the-hardware-question-its-better-than-you-think">The Hardware Question (It’s Better Than You Think)</h2>

<p>This is where people get nervous. Do I need some crazy expensive GPU?</p>

<p>Nope. The actual requirement is super reasonable. About two gigabytes of RAM per one billion model parameters. So a seven billion parameter model needs roughly fourteen gigabytes of RAM. Most laptops today have this. Seriously. Mac users have a genuine advantage. M1, M2, M3 chips are phenomenal for this. Unified memory means CPU and GPU share resources. A MacBook Pro with sixteen gigabytes of unified memory handles seven billion models at speeds comparable to high end gaming GPUs. M3 Max running Llama 3.2 actually competes with RTX 4090 performance. Real tested performance. Not theoretical. Windows users benefit from NVIDIA GPUs if you have them. Models run two to five times faster with proper VRAM. But honestly, even CPU inference works. Slower, but it works.</p>

<p>Here’s the magic part: quantization. This is a compression technique that reduces model precision. Instead of thirty two bit numbers, you use eight or four bit. Model size crashes. A thirty gigabyte model becomes five gigabytes. Ninety percent reduction. And here’s the kicker: it keeps ninety five percent of the capability.</p>

<p>So a seven billion parameter model that normally needs thirty gigs compresses to four gigs. Suddenly state of the art AI runs on hardware you already own.</p>

<h2 id="actually-setting-this-up">Actually Setting This Up</h2>

<p>Okay let’s do this for real. You’ll have a working local LLM in fifteen minutes.</p>

<p><strong>Step one:</strong> Get Ollama. Go to Ollama.com and download the installer for whatever you’re running. Mac. Windows. Linux. All there. Run the installer. Accept defaults. It starts automatically.</p>

<p><strong>Step two:</strong> Pull a model. Open terminal or command prompt. Type this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama pull mistral
</code></pre></div></div>

<p>That’s it. Ollama downloads Mistral 7B. It’s about four gigabytes so depending on your internet connection this takes maybe five to ten minutes. Get coffee. Seriously.</p>

<p><strong>Step three:</strong> Run it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama run mistral
</code></pre></div></div>

<p>You’re chatting with Mistral now. Type a question. Get an answer. Locally. On your laptop. No internet. No API. Just pure LLM running on your machine.</p>

<p><strong>Step four</strong> and optional but really worth doing: Get LM Studio. Download from LM Studio’s website. Same process. Install it. Open it. Find the model you just downloaded and link it. Now you’ve got a beautiful interface running on top of your local model.</p>

<p>That’s literally it. You’ve got a functioning private AI.</p>

<h2 id="what-you-say-in-the-interview">What You Say In The Interview</h2>

<p>You’re in that technical interview and they ask: How do you handle AI?</p>

<p>You don’t mumble about ChatGPT and hope nobody asks about security. You say something like:</p>

<p>“We deploy local language models for anything handling sensitive data or proprietary information. We typically use Mistral 7B or Llama 3.2 running on Ollama. This gives us complete data privacy, offline capability, and eliminates vendor dependency. For tasks that don’t involve sensitive data and need cutting edge capability, we use cloud APIs. But for everything internal, everything stays on our infrastructure and secure.”</p>

<p>That answer? That gets you to the next round. You’re not throwing money at a problem. You’re making intentional architectural decisions. People respect that.</p>

<h2 id="this-is-the-actual-trend">This Is The Actual Trend</h2>

<p>Watch what’s happening. Open source models improve every single quarter. Quantization techniques get better. Hardware becomes more accessible. Meanwhile cloud LLM pricing isn’t dropping. It’s staying high because market dynamics favor providers.</p>

<p>But organizations are waking up. Privacy focused startups choose local deployment. Enterprises with compliance requirements abandon cloud APIs. Researchers needing reproducibility go local. The network effect is real. More people using local means more tools, better docs, stronger community.</p>

<p>That future isn’t coming someday. It’s happening now. Running on your laptop.</p>

<p>You don’t even have to think about it as a ChatGPT replacement anymore. It’s not either or. It’s about matching tools to problems. Cloud for convenience when data doesn’t matter. Local for everything else. Because now you have that option.</p>

<p>So why not try it. Worst case: spend fifteen minutes and have a cool thing to talk about. Best case: you eliminate a two hundred dollar monthly bill and gain complete control over your AI setup.</p>

<p>The future of AI isn’t just in clouds. It’s on your machine. Start right now.</p>

<hr />

<p>Subscribe to Gnanesh Balusa’s blog for more on open source AI, model optimization, and building on edge. Get the latest posts delivered to your inbox.</p>

<p><strong>Subscribe via RSS:</strong> Stay updated with the latest posts by subscribing to the <a href="/gnaneshblog/feed.xml" target="_blank">RSS feed</a>.</p>

<p>Published on November 6, 2025 at 9:18 PM IST</p>]]></content><author><name>Gnanesh Balusa</name></author><category term="ai" /><category term="local-llm" /><category term="ollama" /><category term="privacy" /><category term="chatgpt-alternative" /><category term="llama" /><category term="mistral" /><category term="open-source-ai" /><category term="edge-computing" /><summary type="html"><![CDATA[Stop paying for cloud AI. Run private LLMs locally with Ollama in minutes. Full control, zero cost, complete privacy. Here's how.]]></summary></entry><entry><title type="html">The Quantization Question That Breaks GenAI Engineers in Amazon Interviews</title><link href="http://localhost:4000/gnaneshblog/genai-engineering/2025/11/06/The-Quantization-Question-That-Breaks-GenAI-Engineers-in-Amazon-Interviews.html" rel="alternate" type="text/html" title="The Quantization Question That Breaks GenAI Engineers in Amazon Interviews" /><published>2025-11-06T20:30:00+05:30</published><updated>2025-11-06T20:30:00+05:30</updated><id>http://localhost:4000/gnaneshblog/genai-engineering/2025/11/06/The%20Quantization%20Question%20That%20Breaks%20GenAI%20Engineers%20in%20Amazon%20Interviews</id><content type="html" xml:base="http://localhost:4000/gnaneshblog/genai-engineering/2025/11/06/The-Quantization-Question-That-Breaks-GenAI-Engineers-in-Amazon-Interviews.html"><![CDATA[<p>You’re sitting across from an interviewer at Amazon. The room’s quiet except for the keyboard clicks. Then they drop it:</p>

<p>“We’re building a recommendation engine using a 70B parameter LLM on AWS. Should we use 4-bit or 8-bit quantization? Justify your choice. Also, which EC2 instance types would you recommend?”</p>

<p>Your heart skips. You know what quantization is, right? Reduce the model size. Cool. But that answer? That’s a junior developer answer. And you just watched half the room bomb this exact question.</p>

<p>Here’s the thing most candidates fumble because they only know the surface-level definition. “Quantization reduces model size.” That’s like saying a car is something with wheels. Technically true, but useless.</p>

<p>The engineers who get the offer? They understand that quantization is about tradeoffs. Deep ones. The kind that determine whether your model runs efficiently on AWS SageMaker or ends up costing the company thousands in wasted compute every month.</p>

<h2 id="the-five-things-you-need-to-know-cold">The Five Things You Need to Know Cold</h2>

<h3 id="1-the-precision-performance-tradeoff-the-fundamental-difference">1. The Precision-Performance Tradeoff: The Fundamental Difference</h3>

<p>This is where most people get it wrong. They think smaller always means better. Nope.</p>

<p><strong>8-bit (INT8)</strong> maintains near-identical accuracy. We’re talking performance degradation under 1% on most tasks. It uses linear quantization, which means the conversion is straightforward:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Q = round(scale × W + zero_point)
</code></pre></div></div>

<p>That linearity matters. It’s predictable. If you’re running recommendation systems or search ranking at Amazon scale, that 1% degradation is almost invisible to users.</p>

<p><strong>4-bit (INT4/NF4)</strong> trades accuracy for efficiency. You’re looking at 2-5% performance degradation depending on the architecture. It uses non-linear quantization specifically something called NormalFloat4 which tries hard to preserve the distribution of weights, but it’s fighting an uphill battle. Ever wonder why companies like Google with their LLaMA and Meta use 4-bit more often? They’ve got the infrastructure and validation pipelines to catch quality issues. You might not.</p>

<p>Here’s the brutal truth that interviewers at Amazon want to hear: <strong>8-bit is production-safe. 4-bit requires extensive validation and monitoring in production.</strong></p>

<p>For a recommendation engine? Getting recommendations wrong kills engagement. Kill engagement and revenue drops. That’s the conversation you need to have.</p>

<h3 id="2-the-memory-footprint-where-90-of-engineers-go-wrong">2. The Memory Footprint: Where 90% of Engineers Go Wrong</h3>

<p>Everyone thinks they understand this part. They don’t.</p>

<p>Most people say “4-bit is 2x smaller than 8-bit.” Wrong move. Let me show you the actual numbers that matter for AWS deployment.</p>

<p>Starting with <strong>FP32 (full precision)</strong>: a 70B model takes up 280GB. You’d need multiple <code class="language-plaintext highlighter-rouge">p4d.24xlarge</code> instances just for storage.</p>

<p><strong>INT8</strong> gets you to 70GB. That’s 4x compression. Nice. You’re looking at a single <code class="language-plaintext highlighter-rouge">p3.8xlarge</code> or smaller <code class="language-plaintext highlighter-rouge">g4dn</code> instance family.</p>

<p><strong>INT4</strong> gets you to 35GB. That’s 8x compression. Even nicer, right? Here’s where people get excited and make mistakes.</p>

<p>But you’re not just storing the weights. You’ve still got overhead. Serious overhead. The KV cache (that’s the key-value pairs stored during inference), the activations as data flows through the model, attention buffers… that all adds up fast.</p>

<p>Real-world 70B INT4 deployment on SageMaker? You’re looking at <strong>48-60GB minimum</strong>. Not 35GB. Not even close.</p>

<p>So when your infrastructure team suggests a <code class="language-plaintext highlighter-rouge">g4dn.xlarge</code> with 24GB, you nod politely and then explain why INT4 on that hardware is going to bottleneck your inference. You’d be running single-batch inference at best. At Amazon scale with millions of users? That’s a non-starter.</p>

<p>An <code class="language-plaintext highlighter-rouge">p3.8xlarge</code> with 32GB? A <code class="language-plaintext highlighter-rouge">p4d.24xlarge</code>? Those fit a 70B INT8 model comfortably with room to breathe for batches of 8-16 concurrent requests. That changes the economics of your entire SageMaker deployment and your cost per inference.</p>

<h3 id="3-the-quantization-method-the-hidden-production-killer">3. The Quantization Method: The Hidden Production Killer</h3>

<p>Here’s what separates junior engineers from senior ones. Junior devs pick a quantization method and hope it works. Senior engineers know the methods matter more than the bit depth.</p>

<p><strong>Post-Training Quantization (PTQ)</strong> is fast. You’re talking hours, maybe a day. It works reliably for 8-bit. Companies like Google DeepMind often use PTQ for their initial experiments. For 4-bit? The quality bounces all over the place. Sometimes it’s fine. Sometimes it’s a disaster. And you won’t know which until you’re in production.</p>

<p><strong>GPTQ and AWQ</strong> (these are advanced PTQ methods developed by researchers at UC Berkeley and MIT) are the industry standard for 4-bit LLMs. They use weight-only quantization combined with calibration. But here’s the catch you need a representative calibration dataset. Not just any data. Representative data that actually looks like what your model will see in production.</p>

<p>At Amazon, if you’re building a recommendation engine, your calibration set needs to look like real user queries and product catalogs. Get this wrong and your model will fail spectacularly when it sees real data. Your recommendation engine starts suggesting random products, and suddenly you’ve got a business problem, not just a technical problem.</p>

<p><strong>Quantization-Aware Training (QAT)</strong> is the gold standard. You’re training the model knowing it’ll be quantized, so the weights adapt during training. Companies like Microsoft with their Phi models use QAT extensively. But this costs compute. Days or weeks of GPU time. It’s expensive. It’s rarely done unless the stakes are genuinely high.</p>

<p>For a recommendation system at Amazon scale? The stakes are high. But the question becomes: do you have the compute budget for QAT? If yes, do it. If no, stick with 8-bit and sleep better at night.</p>

<h3 id="4-the-inference-speed-tradeoff-the-counterintuitive-reality">4. The Inference Speed Tradeoff: The Counterintuitive Reality</h3>

<p>Everyone assumes 4-bit is faster because it’s smaller. Welcome to the world where intuition breaks.</p>

<p><strong>8-bit has native GPU support.</strong> Your Tensor Cores (the specialized hardware on modern GPUs like those on AWS <code class="language-plaintext highlighter-rouge">p3</code> and <code class="language-plaintext highlighter-rouge">p4d</code> instances) can handle <code class="language-plaintext highlighter-rouge">INT8</code> operations natively. Matrix multiplication runs at maximum speed. You get 1.5-2x throughput compared to <code class="language-plaintext highlighter-rouge">FP16</code> (half precision). This is why NVIDIA pushed so hard on INT8 support in Ampere and Hopper architectures.</p>

<p><strong>4-bit doesn’t have that native support</strong> on most hardware. The GPU has to dequantize the weights back to <code class="language-plaintext highlighter-rouge">FP16</code> before it can do the actual computation. That dequantization step costs time and bandwidth. You’re memory-bandwidth bound, not compute-bound. The counterintuitive reality? 4-bit often isn’t faster despite being smaller.</p>

<p>In some cases, 4-bit is actually slower than 8-bit for inference because of all that dequantization overhead. This is why companies like Google Cloud and Azure often default to 8-bit for latency-critical workloads.</p>

<p>That matters when you’re trying to serve recommendations in real-time. Every millisecond counts when you’ve got users waiting.</p>

<h3 id="5-the-deployment-reality-the-costs-nobody-talks-about">5. The Deployment Reality: The Costs Nobody Talks About</h3>

<p>This is where theory meets infrastructure, and where most candidates fall apart in interviews.</p>

<p><strong>8-bit scenario:</strong> A <code class="language-plaintext highlighter-rouge">p3.8xlarge</code> on AWS fits a 70B model comfortably. You can run batch sizes of 8-16. Multiple requests can be served together. Your cost per inference goes down significantly. Your latency stays reasonable even under load. Monthly cost for on-demand: around $12,000. Predictable. Manageable.</p>

<p><strong>4-bit scenario:</strong> A <code class="language-plaintext highlighter-rouge">g4dn.12xlarge</code> with 48GB can technically run a 70B model. The word “technically” does a lot of heavy lifting here. You’re looking at batch size 1-4 maximum. One or two requests at a time, essentially. Your throughput tanks. Your cost per inference skyrockets. You might save on compute hours, but you need way more instances to handle the same load. Monthly cost suddenly becomes $40,000+ because you’re running more instances at lower utilization.</p>

<p>The math gets harsh fast. If you need to serve millions of recommendations per day (which Amazon definitely does), 4-bit on smaller EC2 instances isn’t happening. You’d need a farm of high-end instances anyway, and suddenly your “cost savings” from quantization disappear into AWS bills.</p>

<p>This is exactly the kind of thinking that separates people who understand cloud economics from those who just understand machine learning.</p>

<h2 id="so-when-do-you-actually-pick-each-one">So When Do You Actually Pick Each One?</h2>

<p><strong>Go with 8-bit when:</strong></p>

<ul>
  <li>Latency is critical (recommendations, search ranking, real-time predictions)</li>
  <li>You need batch inference with reasonable throughput</li>
  <li>Your team doesn’t have extensive ML ops expertise for validation</li>
  <li>You want predictable AWS costs</li>
  <li>Production reliability matters more than squeezing every last bit of efficiency</li>
</ul>

<p><strong>Go with 4-bit when:</strong></p>

<ul>
  <li>Extreme memory constraints force your hand</li>
  <li>Cost per inference is the only metric that matters</li>
  <li>You can tolerate 2-5% quality degradation</li>
  <li>You’re using GPTQ or AWQ with extensive calibration</li>
  <li>Your team has time and resources to validate thoroughly in staging</li>
  <li>You’re at a company like Google or Meta with massive ML infrastructure</li>
</ul>

<h2 id="the-answer-they-want-to-hear-at-amazon">The Answer They Want to Hear at Amazon</h2>

<p>So you’re in that interview. What do you actually say?</p>

<p>“For an Amazon recommendation engine, I’d go with 8-bit quantization. Here’s why:</p>

<p>First, the accuracy hit. 8-bit gives us less than 1% degradation. In recommendations, that’s acceptable. We might lose one or two good suggestions per user session, which is barely noticeable. 4-bit’s 2-5% degradation means we might be recommending significantly less relevant products. That kills engagement and revenue.</p>

<p>Second, the AWS hardware reality. A <code class="language-plaintext highlighter-rouge">p3.8xlarge</code> or <code class="language-plaintext highlighter-rouge">p4d.24xlarge</code> handles 70B INT8 comfortably with batch sizes of 8-16. That means we can serve multiple users’ requests concurrently. 4-bit on <code class="language-plaintext highlighter-rouge">g4dn</code> instances gives us batch size 1-4 max. We’d need to spin up way more instances to handle the same throughput, which actually costs more money, not less.</p>

<p>Third, latency. 8-bit has native Tensor Core support on AWS P3 and P4D instances. 4-bit requires dequantization overhead. For something as time-sensitive as real-time recommendations where users are waiting for results, 8-bit is measurably faster. We’re talking 20-40ms latency difference, which matters at scale.</p>

<p>Finally, operational simplicity. 8-bit is battle-tested across the industry. Most SageMaker examples use it. Most monitoring tools understand it. We can put this into production confidently next week. 4-bit would require us to build custom calibration pipelines, extensive validation in staging, and more sophisticated monitoring to catch accuracy degradation. That’s weeks of work.</p>

<p>Now, if memory was genuinely the bottleneck and we had to use 4-bit say we only had access to <code class="language-plaintext highlighter-rouge">g4dn</code> instances we’d use GPTQ with our actual recommendation logs as calibration data, run A/B tests extensively before scaling, and probably invest in QAT down the line. But given we’ve got AWS infrastructure available? 8-bit is the right call.”</p>

<p>That’s the answer that gets you to the next round.</p>

<h2 id="the-real-takeaway">The Real Takeaway</h2>

<p>Quantization isn’t just about model size. It’s about understanding the entire ecosystem your AWS hardware options, your latency requirements, your accuracy tolerance, and your cloud budget. Pick the wrong one and you’re either wasting money or losing money. Maybe both.</p>

<p>The engineers who get hired at Amazon, Google, or Microsoft know that the technical choice isn’t separate from the business choice. They’re the same thing. They understand that a quantization decision is actually an infrastructure decision, which is actually a revenue decision. That’s what separates the people who design systems that actually work in production from those who just know the theory.</p>

<p>And that’s the difference between getting an offer and getting a polite “we’ll be in touch.”</p>

<hr />

<p>For more insights on GenAI engineering, AWS deployment, interviews at tech companies, and what it actually takes to deploy LLMs at scale, check out Gnanesh Balusa’s blog.</p>

<p>Published on November 6, 2025 at 8:30 PM IST <a href="/gnaneshblog/feed.xml" target="_blank">RSS feed</a>.</p>]]></content><author><name>Gnanesh Balusa</name></author><category term="genai-engineering" /><category term="quantization" /><category term="llm" /><category term="genai" /><category term="amazon" /><category term="interviews" /><category term="production-deployment" /><category term="deep-learning" /><category term="aws" /><category term="machine-learning" /><summary type="html"><![CDATA[Amazon asks you to deploy a 70B LLM for production systems. 4-bit or 8-bit quantization? Most GenAI engineers freeze. Here's what separates the candidates who actually know their stuff from those who bomb the interview.]]></summary></entry><entry><title type="html">JSONC: The Little Format That Could (Maybe?)</title><link href="http://localhost:4000/gnaneshblog/web-development/2025/11/05/JSONC-The-Little-Format-That-Could-(Maybe).html" rel="alternate" type="text/html" title="JSONC: The Little Format That Could (Maybe?)" /><published>2025-11-05T21:45:00+05:30</published><updated>2025-11-05T21:45:00+05:30</updated><id>http://localhost:4000/gnaneshblog/web-development/2025/11/05/JSONC-The%20Little%20Format%20That%20Could%20(Maybe)</id><content type="html" xml:base="http://localhost:4000/gnaneshblog/web-development/2025/11/05/JSONC-The-Little-Format-That-Could-(Maybe).html"><![CDATA[<p>That’s exactly the problem JSONC was born to solve.</p>

<p>JSON has become the go-to format for everything from <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON" target="_blank">API responses</a> to game save files to configuration. It’s everywhere. But a lot of developers feel like it’s missing some pretty basic features. JSONC is one of several attempts to fix that.</p>
<h2 id="what-even-is-jsonc">What Even Is JSONC?</h2>

<p>Let’s start with the basics. <a href="https://www.json.org/" target="_blank">JSON (JavaScript Object Notation)</a> is a human-readable text format for storing structured data. It uses name-value pairs, arrays, and scalar values. The syntax comes from JavaScript, but these days pretty much every programming language can work with it. That’s the whole point. It’s supposed to be universal.But here’s where things get annoying. JSON has some rules that feel… unnecessarily strict: No comments (you can’t use <code class="language-plaintext highlighter-rouge">//</code> or <code class="language-plaintext highlighter-rouge">/* */</code> to explain anything), no trailing commas (that comma after the last item in a list will break your entire file), keys must be double-quoted always with no exceptions, and multiline strings don’t work (if you want text that wraps across lines, you’re out of luck).</p>

<p>So if you write something like this:</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
</span><span class="err">//</span><span class="w"> </span><span class="err">One</span><span class="w"> </span><span class="err">of</span><span class="w"> </span><span class="s2">"dark"</span><span class="p">,</span><span class="w"> </span><span class="s2">"light"</span><span class="p">,</span><span class="w"> </span><span class="err">or</span><span class="w"> </span><span class="s2">"auto"</span><span class="w">
</span><span class="err">theme:</span><span class="w"> </span><span class="s2">"light"</span><span class="p">,</span><span class="w">

</span><span class="err">description:</span><span class="w"> </span><span class="s2">"This is a description
wrapped over multiple lines for ease
of reading"</span><span class="p">,</span><span class="w">

</span><span class="err">//</span><span class="w"> </span><span class="err">I</span><span class="w"> </span><span class="err">like</span><span class="w"> </span><span class="err">LOADS</span><span class="w"> </span><span class="err">of</span><span class="w"> </span><span class="err">spaces!</span><span class="w">
</span><span class="err">tab-width:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<p>JSON will reject it completely. Instead, you’re stuck with this:</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
</span><span class="nl">"theme"</span><span class="p">:</span><span class="w"> </span><span class="s2">"light"</span><span class="p">,</span><span class="w">
</span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"This is a description wrapped over multiple lines for ease of reading"</span><span class="p">,</span><span class="w">
</span><span class="nl">"tab-width"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<p>Not terrible, but definitely less readable. And good luck remembering what that tab-width setting was for six months from now.</p>

<h2 id="enter-jsonc">Enter JSONC</h2>

<p>JSONC stands for “JSON with Comments.” And yeah, that’s pretty much what it does. It takes regular JSON and adds support for comments. That’s the main thing.</p>

<p>Here’s what you can do with JSONC:</p>

<figure class="highlight"><pre><code class="language-jsonc" data-lang="jsonc"><span class="p">{</span><span class="w">
</span><span class="c1">// One of "dark", "light", or "auto"</span><span class="w">
</span><span class="nl">"theme"</span><span class="p">:</span><span class="w"> </span><span class="s2">"light"</span><span class="p">,</span><span class="w">

</span><span class="err">/*</span><span class="w">
</span><span class="err">This</span><span class="w"> </span><span class="err">is</span><span class="w"> </span><span class="err">a</span><span class="w"> </span><span class="err">block</span><span class="w"> </span><span class="err">comment.</span><span class="w">
</span><span class="err">You</span><span class="w"> </span><span class="err">can</span><span class="w"> </span><span class="err">write</span><span class="w"> </span><span class="err">as</span><span class="w"> </span><span class="err">much</span><span class="w"> </span><span class="err">as</span><span class="w"> </span><span class="err">you</span><span class="w"> </span><span class="err">want</span><span class="w"> </span><span class="err">here.</span><span class="w">
</span><span class="err">Multiple</span><span class="w"> </span><span class="err">lines?</span><span class="w"> </span><span class="err">No</span><span class="w"> </span><span class="err">problem.</span><span class="w">
</span><span class="err">*/</span><span class="w">
</span><span class="nl">"tab-width"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<p>JSONC supports two types of comments: inline comments with <code class="language-plaintext highlighter-rouge">//</code> that run until the end of the line, and block comments with <code class="language-plaintext highlighter-rouge">/* */</code> that can span multiple lines. What it doesn’t support is the <code class="language-plaintext highlighter-rouge">#</code> style comments you’d use in bash scripts. Those won’t work here.</p>

<h2 id="but-wait-there-are-alternatives">But Wait, There Are Alternatives</h2>
<p>Before you get too excited about JSONC, you should know there are other options out there trying to improve JSON.</p>

<h3 id="yaml">YAML</h3>

<p><a href="https://yaml.org/" target="_blank">YAML</a> is probably the most popular alternative. It makes significant changes to JSON’s syntax. Instead of curly braces, it uses indentation. It has custom data types and handles multiline strings naturally. You’ll see YAML used in <a href="https://docs.docker.com/compose/" target="_blank">Docker Compose</a>, <a href="https://docs.github.com/en/actions" target="_blank">GitHub Actions</a> workflows, and Ruby on Rails. YAML is powerful, but it’s also a bigger departure from JSON. Sometimes that’s exactly what you need. Other times, it’s overkill.</p>

<h3 id="json5">JSON5</h3>

<p><a href="https://json5.org/" target="_blank">JSON5</a> aims to be valid JavaScript while fixing pretty much everything people complain about in JSON. It allows trailing commas, unquoted keys, multiline strings, and comments. It’s used in Chromium, Next.js, and macOS programming. JSON5 is more ambitious than JSONC. It tries to fix everything. JSONC, on the other hand, stays focused on one thing: comments.</p>

<h2 id="why-comments-actually-matter-a-lot">Why Comments Actually Matter (A Lot)</h2>
<p>Okay, so comments might not sound revolutionary. But think about it. When was the last time you opened a config file and immediately understood every setting? Here’s where comments really shine:</p>

<h3 id="configuration-files">Configuration Files</h3>

<p>This is the big one. Config files benefit massively from comments. You can add examples right in the file, explain why you chose certain settings, document what each option does, and leave notes for your future self (or your teammates). Instead of maintaining a separate README that people might not find, the documentation lives right there in the config file itself.</p>

<h3 id="data-annotation">Data Annotation</h3>

<p>When you’re storing data that different parts of your code interact with, comments let you explain what that data represents and how it all connects. It’s like leaving breadcrumbs for anyone who comes after you.</p>

<h3 id="temporary-debugging">Temporary Debugging</h3>

<p>You can comment out sections of a file for testing, though honestly this is less common with data files. It can get confusing fast if you’re passing data between systems.</p>

<h2 id="the-other-side-of-the-argument">The Other Side of the Argument</h2>
<p>Now, Douglas Crockford—the guy who originally created JSON—has opinions about comments. He deliberately removed them from JSON. Here’s what he said: “I removed comments from JSON because I saw people were using them to hold parsing directives, a practice which would have destroyed interoperability.” Fair point. Comments can be misused. If people start stuffing parsing instructions into comments, it breaks the whole idea of JSON being a neutral data format. So there’s a legitimate argument for leaving them out. But for config files that aren’t being passed between systems? Comments are incredibly useful.</p>

<h2 id="the-downsides-because-nothings-perfect">The Downsides (Because Nothing’s Perfect)</h2>

<p>JSONC sounds pretty good so far, right? But it’s not without problems.</p>

<h3 id="its-not-that-popular">It’s Not That Popular</h3>

<p>This is the biggest issue. Most tools expect plain JSON, and they’ll fail if you feed them a JSONC file. JSONC is technically a superset of JSON (so all valid JSON is also valid JSONC), but it doesn’t work the other way around. If your tooling doesn’t support JSONC, you’re stuck. And most tooling doesn’t.</p>

<h3 id="it-doesnt-fix-other-annoyances">It Doesn’t Fix Other Annoyances</h3>

<p>JSONC only adds comments. It doesn’t fix trailing commas. It doesn’t allow unquoted keys. If we’re going to create a new format anyway, why not fix everything? Trailing commas make code easier to maintain. You can add or remove items without worrying about comma placement. Git diffs look cleaner. But JSONC doesn’t support them.</p>

<h3 id="converting-jsonc-to-json-is-awkward">Converting JSONC to JSON Is Awkward</h3>
<p>There’s a tool called <a href="https://www.crockford.com/jsmin.html" target="_blank">JSMin</a> (by Douglas Crockford) that can strip comments from JSONC and convert it to plain JSON. But it’s not as simple as you’d think. Because JSONC uses <code class="language-plaintext highlighter-rouge">//</code> and <code class="language-plaintext highlighter-rouge">/* */</code> for comments (the same syntax as code), a converter needs to be a full JSON parser. It has to understand context to avoid accidentally removing something important. If JSONC had chosen a simpler comment syntax—like <code class="language-plaintext highlighter-rouge">#</code> in the first column—conversion would be trivial. You could do it with a single line of bash:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;input.jsonc <span class="nb">grep</span> <span class="nt">-v</span> <span class="s1">'^#'</span> <span class="o">&gt;</span>output.json</code></pre></figure>

<p>But with the current syntax, you need specialized tools.</p>

<h2 id="how-to-actually-use-jsonc-today">How to Actually Use JSONC Today</h2>

<p>So where will you encounter JSONC in the wild? Usually in configuration files. The <a href="https://github.com/fastfetch-cli/fastfetch" target="_blank">fastfetch</a> program, for example, uses JSONC for its config. You can annotate your settings with notes, explanations, whatever you need.</p>

<p>If you’re building something that needs to support JSONC, you’ve got two main approaches:</p>

<h3 id="option-1-convert-it">Option 1: Convert It</h3>

<p>Use a tool like JSMin to strip comments and convert JSONC to plain JSON. Then handle it like you would any other JSON file. This works, but it adds an extra step to your workflow.</p>

<h3 id="option-2-use-a-library">Option 2: Use a Library</h3>

<p>Grab a library that natively supports JSONC. For C, check out <a href="https://github.com/ibireme/yyjson" target="_blank">yyjson</a> which handles JSONC comments plus trailing commas and other nice features. For JavaScript, Microsoft’s <a href="https://github.com/microsoft/node-jsonc-parser" target="_blank">node-jsonc-parser</a> does the job and also supports trailing commas. Pick whichever fits better into your stack.</p>

<h2 id="so-should-you-use-jsonc">So Should You Use JSONC?</h2>
<p>Here’s the truth: JSONC solves a specific problem really well. If you’re working with config files and wish you could add documentation inline, JSONC is perfect for that. But it’s not a universal JSON replacement. It doesn’t fix all of JSON’s quirks. And most importantly, it’s not widely supported. You’ll need to handle conversion or use specialized libraries if you want to integrate with standard JSON tooling.</p>

<p>If you’re building something new and you control the entire stack, JSONC is worth considering. For config files especially, the ability to add comments is legitimately valuable. But if you’re working in an ecosystem that expects standard JSON? Stick with JSON. It’s been working great for years, and there’s no shame in that.</p>

<h2 id="the-big-takeaway">The Big Takeaway</h2>

<p>JSONC isn’t trying to replace JSON for everything. It’s trying to make config files and annotated data more maintainable. For that specific use case, it succeeds. Would it be better if it also fixed trailing commas and unquoted keys? Probably. Would it be better if conversion to JSON was simpler? Definitely. But for what it is—JSON with comments—it does the job. And sometimes, that’s exactly what you need.</p>

<p>For more insights on web development, programming, and tech, check out Gnanesh Balusa’s blog.</p>

<p>Published on November 5, 2025 at 9:45 PM IST</p>]]></content><author><name>Gnanesh Balusa</name></author><category term="web-development" /><category term="json" /><category term="jsonc" /><category term="data-formats" /><category term="configuration" /><category term="programming" /><summary type="html"><![CDATA[Ever wondered why you can't add comments to your JSON files? JSONC fixes that. But is it enough? Let's dig into what JSONC actually does, why it exists, and whether you should care]]></summary></entry></feed>